{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55766981",
   "metadata": {},
   "source": [
    "# Data Clean up\n",
    "Updated on: 04/08/2022\n",
    "\n",
    "Authors: Abzer Kelminal (abzer.shah@uni-tuebingen.de) <br>\n",
    "Input file format: .csv files or .txt files <br>\n",
    "Outputs: .csv files  <br>\n",
    "Dependencies: ggplot2, dplyr, IRdisplay\n",
    "\n",
    "This Notebook is used for cleaning the feature table, an output of metabolomics experiment, containing all the features with their corresponding intensities. The data cleanup steps involved are: 1) Blank removal 2) Imputation 3) Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5aebdb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Kindly download this Jupyter Notebook and run it in your local computer for successful running of the code. To know more about how to get the Jupyter Notebook running with R code, please have a look at this document: [GitHub Link](https://github.com/Functional-Metabolomics-Lab/Jupyter-Notebook-Installation/blob/main/Anaconda%20with%20R%20kernel%20installation.pdf)\n",
    "    \n",
    "    \n",
    "<b><font size=3> SPECIAL NOTE: Please read the comments before proceeding with the code and let us know if you run into any errors and if you think it could be commented better. We would highly appreciate your suggestions and comments!!</font> </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f59af0-e3d3-41cf-a6d0-e5656c501d8a",
   "metadata": {},
   "source": [
    "# Brief introduction about the Test data:\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "    \n",
    "- <p style='text-align: justify;'> Bacteria (B.subtilis and E.coli) were treated with a pool of antibiotics (Sulfamethoxazole, sulfadimethoxine, cyproconazole) and also a xenobiotic, Asulam, taken at a concentration lower than their MIC (minimum inhibitory concentration). </p>\n",
    "- <p style='text-align: justify;'> The samples were collected at different timepoints, the compounds were extracted (with EtOAc) and measured using LC-MS/MS. </p>\n",
    "- <p style='text-align: justify;'> The goal of the experiment was to look for any potential biotransformation. eg: Drug or xenobiotic metabolism </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4705b9a",
   "metadata": {},
   "source": [
    "# Package installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8048c07",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "#installing and calling the necessary packages:\n",
    "if (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n",
    "if (!require(\"dplyr\")) install.packages(\"dplyr\")\n",
    "if (!require(\"IRdisplay\")) install.packages(\"IRdisplay\")\n",
    "\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(IRdisplay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557e9d6",
   "metadata": {},
   "source": [
    "# Getting the input files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31eafc",
   "metadata": {},
   "source": [
    "## 1) Reading the input data using URL (from GitHub):\n",
    "Here, we can directly pull the data files from our Functional Metabolomics Github page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988ffd6",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "## Non-gap filled\n",
    "nft_url <- 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/Metabolomics_SummerSchool_2022/main/data/MZmine/20220726_Xenobiotic_metabolism_Nongapfilled_quant.csv'\n",
    "## Gap filled\n",
    "ft_url <- 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/Metabolomics_SummerSchool_2022/main/data/MZmine/20220726_Xenobiotic_metabolism_Gapfilled_quant.csv'\n",
    "md_url <- 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/Metabolomics_SummerSchool_2022/main/data/20220726_Xenobiotic_Metabolism_metadata.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9e910",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "nft <- read.csv(nft_url, header = T, check.names = F)\n",
    "ft <- read.csv(ft_url, header = T, check.names = F)\n",
    "md <- read.csv(md_url, header = T, check.names = F, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a44d4",
   "metadata": {},
   "source": [
    "## 2) Setting a local working directory and creating an automatic result directory:\n",
    "<p style='text-align: justify;'> <font color='green'> <b> Setting a local working directory works well with Jupyter Notebook.</b> If you are working with Jupyter Notebook, you can simply copy the folder path of your input files from your local computer to the output line of the next cell. It will be set as your working directory (or working folder)  </font></p> \n",
    "For ex: D:\\User\\Project\\Test_Data <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734f4b6",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "# setting the current directory as the working directory\n",
    "Directory <- normalizePath(readline(\"Enter the path of the folder with input files: \"),\"/\",mustWork=FALSE)\n",
    "setwd(Directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0436b28",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "getwd() #to get the working directory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ff905-778f-425b-bdaf-8f8bbdc823aa",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> <font color='red'> <b> For Google Collab, it is not possible to access the files from your local computer as it is hosted on Google's cloud server. An easier workaround is to upload the necessary files into the Google colab session using the 'Files' icon on the left. The code below creates a new folder 'My_TestData' in the Colab space and sets the folder as working directory. </b> </font> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca144ac-b007-41b7-8549-68d349cfa515",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "#Only for Google Colab:\n",
    "dir.create(\"/content/My_TestData\", showWarnings = TRUE, recursive = FALSE, mode = \"0777\")\n",
    "setwd(\"/content/My_TestData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec0036-1378-45d2-bc13-2cde2047e7d4",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> <font color='red'> Now, we can upload our files from our local PC to the folder 'My_TestData' and continue with the rest of the script. </font> </p>\n",
    "<p style='text-align: justify;'> <font color='red'> <b> SPECIAL NOTE: All the files uploaded to Google Colab would generally disappear after 12 hours. Similarly, all the outputs would be saved in the Colab and we need to download them into our local system at the end of our session. </b> </font> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd010938",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "# Getting all the files in the folder\n",
    "dirs <- dir(path=paste(getwd(), sep=\"\"), full.names=TRUE, recursive=TRUE)\n",
    "folders <- unique(dirname(dirs))\n",
    "files <- list.files(folders, full.names=TRUE)\n",
    "files_1 <- basename((files))\n",
    "files_2 <- dirname((files))\n",
    "# Creating a Result folder\n",
    "dir.create(path=paste(files_2[[1]], \"_Results\", sep=\"\"), showWarnings = TRUE)\n",
    "fName <-paste(files_2[[1]], \"_Results\", sep=\"\")\n",
    "\n",
    "print(files_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab1133",
   "metadata": {},
   "source": [
    "In the following line, enter the required file index numbers separated by commas. For example as: 1,2,3. The accepted file formats are csv,txt and tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d07e82",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "input <- as.double(unlist(strsplit(readline(\"Specify the file index of gapfilled feature-file, metadata separated by commas:\"), split=\",\")))\n",
    "\n",
    "#Gets the extension of each file. Ex:csv\n",
    "pattern <- c()\n",
    "for (i in files_1){\n",
    "  sep_file <- substr(i, nchar(i)-2,nchar(i))\n",
    "  pattern <- rbind(pattern,sep_file)\n",
    "}\n",
    "#pattern\n",
    "\n",
    "ft <- read.csv(files_1[input[1]],sep = ifelse(pattern[input[1]]!=\"csv\",\"\\t\",\",\"), header=TRUE,check.names = FALSE) # By applying 'row.names = 1', the 1st column 'ID' becomes the row names\n",
    "md <-read.csv(files_1[input[2]], sep = ifelse(pattern[input[2]]!=\"csv\",\"\\t\",\",\"), header=TRUE,check.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d48db",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> Since non gapfilled feature file is not a direct output of mzMine, not everybody will have it. Therefore, we have the next cell separately. For the imputation step, we can also impute the feature table without a non-gapfilled feature file.</p>\n",
    "\n",
    " More about this in the later section: [Imputation with a Cutoff LOD](#Impute_LOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da9db7",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "#If you have non gapfilled feature file:\n",
    "if(readline(\"Do you have non gap-filled feature table? Y/N:\")==\"Y\"){\n",
    "  x <- as.double(readline(\"Enter the ID number of non-gap-filled feature file:\"))\n",
    "  nft<- read.csv(files_1[x],sep=ifelse(pattern[x]!=\"csv\",\"\\t\",\",\"), header = TRUE,check.names = FALSE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12e24c",
   "metadata": {},
   "source": [
    "Lets check if the data has been read correclty!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ff705",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "head(ft)\n",
    "dim(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b7231",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "head(nft)\n",
    "dim(nft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf0865",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "head(md)\n",
    "dim(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19073e0",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> In the next cell, we are trying to bring the feature table and metadata in the correct format such as the rownames of metadata and column names of feature table are the same. They both are the file names and they need to be the same, as from now on, we will call the columns in our feature table based on our metadata information. Thus, using the metadata, the user can filter their data easily. You can also directly deal with your feature table without metadata by getting your hands dirty with some coding!! But having a metadata improves the user-experience greatly. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffd93c",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "#Removing Peak area extensions\n",
    "colnames(ft) <- gsub(' Peak area','',colnames(ft))\n",
    "md$filename<- gsub(' Peak area','',md$filename)\n",
    "\n",
    "#Removing if any NA columns present in the md file\n",
    "md <- md[,colSums(is.na(md))<nrow(md)]\n",
    "\n",
    "#Changing the row names of the files\n",
    "rownames(md) <- md$filename\n",
    "md <- md[,-1]\n",
    "rownames(ft) <- paste(ft$'row ID',round(ft$'row m/z',digits = 3),round(ft$'row retention time',digits = 3), sep = '_')\n",
    "\n",
    "#Picking only the files with column names containing 'mzML'\n",
    "ft <- ft[,grep('mzML',colnames(ft))]\n",
    "\n",
    "# if nft file exists, we perform all the above for nft as well\n",
    "if(exists(\"nft\")==T){\n",
    "    colnames(nft) <- gsub(' Peak area','',colnames(nft))\n",
    "    rownames(nft) <- paste(nft$'row ID',round(nft$'row m/z',digits = 3),round(nft$'row retention time',digits = 3), sep = '_')\n",
    "    nft <- nft[,grep('mzML',colnames(nft))]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82159eb",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "new_ft<- ft[,order(colnames(ft))] #ordering the ft by its column names\n",
    "new_md <-md[order(rownames(md)),] #ordering the md by its row names\n",
    "\n",
    "#lists the colnames(ft) which are not present in md\n",
    "unmatched_ft <- colnames(new_ft)[which(is.na(match(colnames(new_ft),rownames(new_md))))] \n",
    "cat(\"These\", length(unmatched_ft),\"columns of feature table are not present in metadata:\")\n",
    "if((length(unmatched_ft) %% 2) ==0)\n",
    "{matrix(data=unmatched_ft,nrow=length(unmatched_ft)/2,ncol=2)}else\n",
    "{matrix(data=unmatched_ft,nrow=(length(unmatched_ft)+1)/2,ncol=2)}\n",
    "\n",
    "flush.console()\n",
    "Sys.sleep(0.2)\n",
    "\n",
    "#lists the rownames of md which are not present in ft\n",
    "unmatched_md <- rownames(new_md)[which(is.na(match(rownames(new_md),colnames(new_ft))))] \n",
    "cat(\"These\", length(unmatched_md),\"rows of metadata are not present in feature table:\")\n",
    "if((length(unmatched_md) %% 2) ==0)\n",
    "{matrix(data=unmatched_md,nrow=length(unmatched_md)/2,ncol=2)}else\n",
    "{matrix(data=unmatched_md,nrow=(length(unmatched_md)+1)/2,ncol=2)}\n",
    "\n",
    "#Removing those unmatching columns and rows:\n",
    "if(length(unmatched_ft)!=0){new_ft <- subset(ft, select = -c(which(is.na(match(colnames(ft),rownames(md))))) )}\n",
    "if(length(unmatched_md)!=0){new_md <- md[-c(which(is.na(match(rownames(md),colnames(ft))))),]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881877f6",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "#checking the dimensions of our new ft and md:\n",
    "cat(\"The number of rows and columns in our original ft is:\",dim(ft),\"\\n\")\n",
    "cat(\"The number of rows and columns in our new ft is:\",dim(new_ft),\"\\n\")\n",
    "cat(\"The number of rows and columns in our new md is:\",dim(new_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed594448",
   "metadata": {},
   "source": [
    "Notice that the number of columns of feature table is same as the number of rows in our metadata. Now, we have both our feature table and metadata in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01979e55",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "new_ft<- new_ft[,order(colnames(new_ft))] #ordering the ft by its column names\n",
    "new_md <-new_md[order(rownames(new_md)),] #ordering the md by its row names\n",
    "#checking if they are the same\n",
    "if(identical(colnames(new_ft),rownames(new_md))==T)\n",
    "   {print(\"The column names of ft and rownames of md are the same\")}else{print(\"The column names of ft and rownames of md are not the same\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927cf45",
   "metadata": {},
   "source": [
    "Lets check the files once again!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fb0e3",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "head(nft)\n",
    "dim(nft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d8a48",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "head(new_ft)\n",
    "dim(new_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6027aea7",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "head(md)\n",
    "dim(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45b589",
   "metadata": {},
   "source": [
    "# Splitting the data into Blanks and Samples using Metadata:\n",
    "<a id=\"data_split\"></a>\n",
    "\n",
    "For the first step: Blank removal, we need to split the data as spectra obtained from blanks and samples respectively using the metadata. More about Blank removal in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c9859f",
   "metadata": {},
   "source": [
    "Let's call the metadata and feature table in different names so that we don't change the original files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35b178",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "Meta_Filter <- md\n",
    "input_data <- new_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6210ac",
   "metadata": {},
   "source": [
    "From the below cell output, we get an idea of the multiple levels in each of the metioned attributes in the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de36fa",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    " lev <- c()\n",
    "  for(i in 1:ncol(Meta_Filter)){\n",
    "    x <- levels(as.factor(Meta_Filter[,i]))\n",
    "    if(is.double(Meta_Filter[,i])==T){x=round(as.double(x),2)}\n",
    "    x <-toString(x)\n",
    "    lev <- rbind(lev,x)\n",
    "  }\n",
    "\n",
    "Info_mat <- data.frame(INDEX=c(1:ncol(Meta_Filter)),ATTRIBUTES=colnames(Meta_Filter),LEVELS=lev,row.names=NULL)\n",
    "Info_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aef240",
   "metadata": {},
   "source": [
    "### Batch-wise splitting of the data:\n",
    "\n",
    "In our test dataset, we have used 2 species data: B.sub and E.coli. If you want to split the data batch-wise, run the next cell. For example: We can split it as B.sub + Media and/or E.coli + Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4612530",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "#Splitting batch-wise:\n",
    "Batch_info <- ifelse(readline(\"Do you want to split your data batch-wise? Y/N: \")==\"Y\",as.double(readline(\"Please enter the index of the attributes having batch information:\")),\"No\")\n",
    "if(is.numeric(Batch_info)==TRUE){\n",
    "    Levels_Batch <- levels(as.factor(Meta_Filter[,Batch_info]))\n",
    "    IRdisplay::display(data.frame(INDEX=c(1:length(Levels_Batch)),LEVELS=Levels_Batch))\n",
    "    \n",
    "    flush.console()  \n",
    "    Sys.sleep(0.2)\n",
    "    \n",
    "    Cdtn <-as.double(unlist(strsplit(readline(\"Enter the index number of condition(s) you want to KEEP (separated by commas):\"), split=',')))\n",
    "    \n",
    "    #Getting all the rows of metadata that satisfies each element of the condition and storing it as an element in Split_data list\n",
    "    Split_data <-list()\n",
    "    for (j in 1:length(Cdtn)){\n",
    "      Split_data[[j]] <- Meta_Filter[(Meta_Filter[,Batch_info]==Levels_Batch[Cdtn[j]]),]\n",
    "    }\n",
    "    \n",
    "    Batch_data <-do.call(rbind, Split_data) # unlists the Split data and combines them by row\n",
    "    flush.console()  \n",
    "    Sys.sleep(0.2)\n",
    "   \n",
    "    IRdisplay::display(head(Batch_data)) #Visualising the Batch_data\n",
    "    dim(Batch_data)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe2ef5",
   "metadata": {},
   "source": [
    "Once we subset the data according to a batch, we can further proceed to split the blanks from the sample in the cell below. If no batch data is present, you can simply split your metadata into blank and sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8af3d",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "#If batch data exists, it will take it as \"data\", else take Meta_filter as \"data\"\n",
    "if(exists(\"Batch_data\")==T){data <-Batch_data}else{data <-Meta_Filter}\n",
    "\n",
    "Info_mat\n",
    "Condition <- as.double(unlist(readline(\"Enter the index number of the attribute to split sample and blank:\")))\n",
    "\n",
    "Levels_Cdtn <- levels(as.factor(data[,Condition[1]]))\n",
    "print(matrix(Levels_Cdtn,length(Levels_Cdtn)))\n",
    " \n",
    "  #Among the shown levels of an attribute, select the ones to keep\n",
    "Blk_id <- as.double(unlist(readline(\"Enter the index number of your BLANK:\")))\n",
    "paste0('You chosen blank is:',Levels_Cdtn[Blk_id])\n",
    "  \n",
    "  #Splitting the data into blanks and samples based on the metadata\n",
    "md_Blank <- data[(data[,Condition] == Levels_Cdtn[Blk_id]),]\n",
    "Blank <- input_data[,which(colnames(input_data)%in%rownames(md_Blank))] \n",
    "md_Samples <- data[(data[,Condition] != Levels_Cdtn[Blk_id]),]\n",
    "Samples <- input_data[,which(colnames(input_data)%in%rownames(md_Samples))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed01148",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "head(Blank)\n",
    "dim(Blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37531ef9",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "head(Samples)\n",
    "dim(Samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96275453",
   "metadata": {},
   "source": [
    "# Creating a function named FrequencyPlot:  \n",
    "The below function takes the two input datatables: for example, gapfilled and non-gapfilled as its inputs, calculates the frequency distribution of the data in the order of 10 and produces a grouped barplot showing the distribution as output. The frequency plot shows where the features are present in higher number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2b3b4",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "#'Global' settings for plot size in the output cell\n",
    "#options(repr.plot.width=10, repr.plot.height=8,res=600) #For google collab\n",
    "options(repr.plot.width=5, repr.plot.height=3) #For Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755a764",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "FrequencyPlot <- function(x1,x2){\n",
    "  \n",
    "   #creating bins from -1 to 10^10 using sequence function seq()\n",
    "    bins <- c(-1,0,(1 * 10^(seq(0,10,1)))) \n",
    "    \n",
    "    #cut function cuts the give table into its appropriate bins\n",
    "    scores_x1 <- cut(as.matrix(x1),bins,labels = c('0','1','10','1E2','1E3','1E4','1E5','1E6','1E7','1E8','1E9','1E10')) \n",
    "    \n",
    "    #transform function convert the tables into a column format: easy for visualization \n",
    "    Table_x1<-transform(table(scores_x1)) #contains 2 columns: \"scores_x1\", \"Freq\"\n",
    "    \n",
    "    #Repeating the same steps for x2\n",
    "    scores_x2 <- cut(as.matrix(x2),bins,labels = c('0','1','10','1E2','1E3','1E4','1E5','1E6','1E7','1E8','1E9','1E10'))\n",
    "    Table_x2<-transform(table(scores_x2))\n",
    "  \n",
    "    #Getting the names of x1 and x2\n",
    "    arg1 <- deparse(substitute(x1))\n",
    "    arg2 <-deparse(substitute(x2))\n",
    "    \n",
    "    #Creating a data frame for plotting\n",
    "    data_plot <- as.data.frame(c(Table_x1$Freq,Table_x2$Freq)) #Concatenating the frequency info of both tables rowwise\n",
    "    colnames(data_plot) <- \"Freq\" #naming the 1st column as 'Freq'\n",
    "    data_plot$Condition <- c(rep(arg1,12),rep(arg2,12)) #adding a 2nd column 'Condition', which just repeats the name of x1 and x2 accordingly\n",
    "    data_plot$Range_bins <- rep(Table_x1$scores_x1,2) #Adding 3rd column 'Range Bins'\n",
    "    data_plot$Log_Freq <- log(data_plot$Freq+1) #Log scaling the frequency values\n",
    "    \n",
    "    ## GGPLOT2\n",
    "    BarPlot <- ggplot(data_plot, aes(Range_bins, Log_Freq, fill = Condition)) + \n",
    "    geom_bar(stat=\"identity\", position = \"dodge\", width=0.4) + \n",
    "    scale_fill_brewer(palette = \"Set1\") +\n",
    "    ggtitle(label=\"Frequency plot\") +\n",
    "    xlab(\"Range\") + ylab(\"(Log)Frequency\") + labs(fill = \"Data Type\") + \n",
    "    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +   # setting the angle for the x label\n",
    "    theme(axis.text.y = element_text(angle = 45, vjust = 0.5, hjust=1)) +   # setting the angle for the y label\n",
    "    theme(plot.title = element_text(hjust = 0.5)) # centering the plot title\n",
    "  \n",
    "    print(BarPlot)\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ce796",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> As you have noticed, the above cell didn't produce any outputs. Here, we created our own function called 'FrequencyPlot'. which works similar to \"FREQUENCY\" function in MS Excel. Here, we have also added codes to print a nice ggplot as ouput of our function. Hence, like 'print' function prints an output, whenever we use our 'FrequencyPlot' function with the input variables, it will produce an output plot using the calculation in the above cell. </p>\n",
    "\n",
    "**Now that we have our data ready, we can start with the cleanup steps!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43099574",
   "metadata": {},
   "source": [
    "# Step1: Blank Removal\n",
    "\n",
    "<p style='text-align: justify;'> In LC-MS/MS, we use solvents called Blanks which are usually injected time-to-time to prevent carryover of the sample. The features coming from these Blanks would also be detected by LC-MS/MS instrument. Our goal here is to remove these features from our samples. The other blanks that can be removed are: Signals coming from growth media alone in terms of microbial growth experiment, signals from the solvent used for extraction methods and so on. Therefore, it is best practice to measure mass spectra of these blanks as well in addition to your sample spectra. </p>\n",
    "\n",
    "**How do we remove these blank features?** </br> \n",
    "<p style='text-align: justify;'> Since we have the feature table split into Control blanks and Sample groups now, we can compare blanks to the sample to identify the background features coming from blanks. A common filtering method is to use a cutoff to remove features that are not present sufficient enough in our biological samples. </p>\n",
    "\n",
    "The steps followed in the next few cells are:\n",
    "1. <p style='text-align: justify;'> We find an average for all the feature intensities in your blank set and sample set. Therefore, for n no.of features in a blank or sample set, we get n no.of averaged features. </p>\n",
    "2. <p style='text-align: justify;'> Next, we get a ratio of this average_blanks vs average_sample. This ratio Blank/sample tells us how much of that particular feature of a sample gets its contribution from blanks. If it is more than 30% (or Cutoff as 0.3), we consider the feature as noise. </p>\n",
    "3. <p style='text-align: justify;'> The resultant information (if ratio > Cutoff or not) is stored in a bin such as 1 = Noise or background signal, 0 = Feature Signal</p>\n",
    "4. <p style='text-align: justify;'> We count the no.of features in the bin that satisfies the condition ratio > cutoff, and consider those features as 'noise or background features' and remove them. </p>\n",
    "\n",
    "For a dataset containing several batches, the filtering steps are performed batch-wise and it can be done using the previous section [Batch-wise splitting of the data](#data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eaf47e",
   "metadata": {},
   "source": [
    "**<font color='red'> The Cutoff used to obtain the all the files in MZmine Results folder is 0.3 </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ea36a",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "if(readline('Do you want to perform Blank Removal- Y/N:')=='Y'){\n",
    "    \n",
    "    #When cutoff is low, more noise (or background) detected; With higher cutoff, less background detected, thus more features observed\n",
    "    Cutoff <- as.numeric(readline('Enter Cutoff value between 0.1 & 1:')) # (i.e. 10% - 100%). Ideal cutoff range: 0.1-0.3\n",
    "    \n",
    "    #Getting mean for every feature in blank and Samples\n",
    "    Avg_blank <- rowMeans(Blank, na.rm= FALSE, dims = 1) # set na.rm = FALSE to check if there are NA values. When set as TRUE, NA values are changed to 0\n",
    "    Avg_samples <- rowMeans(Samples, na.rm= FALSE, dims = 1)\n",
    "    \n",
    "    #Getting the ratio of blank vs Sample\n",
    "    Ratio_blank_Sample <- (Avg_blank+1)/(Avg_samples+1)\n",
    "    \n",
    "    # Creating a bin with 1s when the ratio>Cutoff, else put 0s\n",
    "    Bg_bin <- ifelse(Ratio_blank_Sample > Cutoff, 1, 0 )\n",
    "    Blank_removal <- cbind(Samples,Bg_bin)\n",
    "\n",
    "    # Checking if there are any NA values present. Having NA values in the 4 variables will affect the final dataset to be created\n",
    "    temp_NA_Count <-cbind(Avg_blank ,Avg_samples,Ratio_blank_Sample,Bg_bin)\n",
    "    \n",
    "    print('No of NA values in the following columns:')\n",
    "    print(colSums(is.na(temp_NA_Count)))\n",
    "\n",
    "     #Calculating the number of background features and features present\n",
    "    print(paste(\"No.of Background or noise features:\",sum(Bg_bin ==1,na.rm = TRUE)))\n",
    "    print(paste(\"No.of features after excluding noise:\",(nrow(Samples) - sum(Bg_bin ==1,na.rm = TRUE)))) \n",
    "\n",
    "    Blank_removal <- Blank_removal %>% filter(Bg_bin == 0) # Taking only the feature signals\n",
    "    Blank_removal <- as.matrix(Blank_removal[,-ncol(Blank_removal)]) # removing the last column Bg_bin \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbd3f6",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "head(Blank_removal)\n",
    "dim(Blank_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4210007",
   "metadata": {},
   "source": [
    "# Step 2: Imputation\n",
    "\n",
    "<p style='text-align: justify;'> For several reasons, real world datasets might have some missing values in it, in the form of NA, NANs or 0s. Eventhough the gapfilling step of MZmine fills the missing values, we still end up with some missing values or 0s in our feature table. This could be problematic for statistical analysis. </p> \n",
    "<p style='text-align: justify;'> In order to have a better dataset, we cannot simply discard those rows or columns with missing values as we will lose a chunk of our valuable data. Instead we can try imputing those missing values. Imputation involves replacing the missing values in the data with a meaningful, reasonable guess. There are several methods, such as: </p> \n",
    "  \n",
    "1) Mean imputation (replacing the missing values in a column with the mean or average of the column)  \n",
    "2) Replacing it with the most frequent value  \n",
    "3) Several other machine learning imputation methods such as k-nearest neighbors algorithm(k-NN), Hidden Markov Model(HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ee8a6",
   "metadata": {},
   "source": [
    "## 1) Imputation using Non-gapfilled feature table:\n",
    "One such method, we are going to use is: **to replace the zeros from the gapfilled quant table with the non-gap filled table** we get from MZmine. In order to do that, we can visualize our data distribution using the frequenct plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83ae53",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "GapFilled <-Blank_removal\n",
    "if(exists(\"nft\")==T){NotGapFilled <-nft}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527abdb",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "if(readline('Do you want to perform Imputation with minimum value of NonGapFilled table? - Y/N:')=='Y'){\n",
    "    plot<- FrequencyPlot(GapFilled,NotGapFilled)\n",
    "    \n",
    "    Arg1 = plot$data$Condition[1]\n",
    "    Arg2 = plot$data$Condition[13]\n",
    "    \n",
    "    # getting the 2nd minimum value of non-gap filled data. (The first minimum value in the data table is usually zero)\n",
    "    RawLOD <- round(min(NotGapFilled[NotGapFilled!=min(NotGapFilled)]))\n",
    "    \n",
    "    print(paste0(\"The minimum value greater than 0 for \",Arg1,\":\", round(min(GapFilled[GapFilled!=min(GapFilled)]))))\n",
    "    print(paste0(\"The minimum value greater than 0 for \",Arg2,\":\", RawLOD))\n",
    "\n",
    "    Imputed <- GapFilled\n",
    "    Imputed[Imputed<RawLOD] <- RawLOD # Replacing values<RawLOD with RawLOD\n",
    "    head(Imputed) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4916143",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "write.csv(Imputed, file.path(fName,paste0('Imputed_QuantTable_filled_with_MinValue_',RawLOD,'_CutOff_Used_',Cutoff,'.csv')),row.names =TRUE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06030071",
   "metadata": {},
   "source": [
    "## 2) Imputation with a Cutoff LOD:\n",
    "<a id=\"Impute_LOD\"></a>\n",
    "\n",
    "Instead of replacing with the minimum value of nft, we can also only use ft and see the frquency distribution of its features with the frequency plot. The frequency plot shows where the features are present in higher number.\n",
    "\n",
    "For ex: If until range 10-100, (shown in the figure as 1E2) there are no or very less features, we want to exclude until that range and consider from range (100-1000), or, in other words, '1E3' or '1000' as Cutoff_LOD. This value will be used to replace the zeros in the data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd2928",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "FrequencyPlot(GapFilled,GapFilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8f811",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "if(readline('Do you want to perform Imputation with a Cutoff LOD? - Y/N:')=='Y'){\n",
    "    Cutoff_LOD <-as.numeric(readline(\"Enter your Cutoff LOD as seen in the plot:\"))  #Enter the LOD value as seen in the frequency plot\n",
    "    Imputed <- GapFilled\n",
    "    Imputed[Imputed <Cutoff_LOD] <- Cutoff_LOD\n",
    "    head(Imputed)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67865b8",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "dim(Imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed071fa7",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "write.csv(Imputed,file.path(fName,paste0('Imputed_QuantTable_filled_with_',Cutoff_LOD,'_CutOff_Used_',Cutoff,'_Bsub','.csv')),row.names =TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f428856",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "#removing all the rows with only cutoff values:\n",
    "#Imputed<-Imputed[rowMeans(Imputed)!= Cutoff_LOD,]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a2dd7",
   "metadata": {},
   "source": [
    "# Step 3:Normalization\n",
    "The following code performs sample-centric (column-wise) normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a008a7a",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "if (readline(\"Do you want to perform Normalization: Y/N:\") == 'Y'){\n",
    "    #Getting column-wise sums of the input-data\n",
    "    sample_sum <- colSums(Imputed, na.rm= TRUE, dims = 1)\n",
    "    \n",
    "    #Dividing each element of a particular column with its column sum\n",
    "    Normalized_data <- c()\n",
    "    for (i in 1:ncol(Imputed)){\n",
    "        x <- Imputed[,i] / sample_sum[i]\n",
    "        Normalized_data <- cbind(Normalized_data, x)\n",
    "    }\n",
    "    colnames(Normalized_data) <- names(sample_sum)\n",
    "    \n",
    "    head(Normalized_data)\n",
    "} else return(head(Imputed))\n",
    "  \n",
    "print(paste('No.of NA values in Normalized data:',sum(is.na(Normalized_data)== TRUE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539f80c",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "dim(Normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc407e",
   "metadata": {	
    "vscode": {	
     "languageId": "r"	
    }	
   },
   "outputs": [],
   "source": [
    "write.csv(Normalized_data, file.path(fName,'Normalised_Quant_table_Bsub_OnlyCutOffsRemoved.csv'),row.names =TRUE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
