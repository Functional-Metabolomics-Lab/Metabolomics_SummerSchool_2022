{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55766981",
   "metadata": {},
   "source": [
    "# Data Clean up\n",
    "Authors: Abzer Kelminal (abzer.shah@uni-tuebingen.de) <br>\n",
    "Edited by:  <br>\n",
    "Input file format: .csv files or .txt files <br>\n",
    "Outputs: .csv files  <br>\n",
    "Dependencies: ggplot2, dplyr\n",
    "\n",
    "This Notebook is used for cleaning the feature table, an output of metabolomics experiment, containing all the features with their corresponding intensities. The data cleanup steps involved are: 1) Blank removal 2) Imputation 3) Normlisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55115ca7",
   "metadata": {},
   "source": [
    "# Brief introduction about the Test data:\n",
    "- <p style='text-align: justify;'> Bacteria (B.subtilis and E.coli) were treated with a pool of antibiotics (Sulfamethoxazole, sulfadimethoxine, cyproconazole) and also a xenobiotic, Asulam, taken at a concentration lower than their MIC (minimum inhibitory concentration). </p>\n",
    "- <p style='text-align: justify;'> The samples were collected at different timepoints, the compounds were extracted (with 50% EtOAc) and measured using LC-MS/MS. </p>\n",
    "- <p style='text-align: justify;'> The goal of the experiment was to look for any potential biotransformation. eg: Drug or xenobiotic metabolism </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e43fe3",
   "metadata": {},
   "source": [
    "# Package installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8048c07",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#installing and calling the necessary packages:\n",
    "if (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n",
    "if (!require(\"dplyr\")) install.packages(\"dplyr\")\n",
    "\n",
    "library(ggplot2)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7085bb",
   "metadata": {},
   "source": [
    "# Getting the input files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31eafc",
   "metadata": {},
   "source": [
    "## 1) Reading the input data using URL (from GitHub):\n",
    "Here, we can directly pull the data files from our Functional Metabolomics Github page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988ffd6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Non-gap filled\n",
    "nft_url <- 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/Metabolomics_SummerSchool_2022/main/data/MZmine/20220726_Xenobiotic_metabolism_Nongapfilled_quant.csv'\n",
    "## Gap filled\n",
    "ft_url <- 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/Metabolomics_SummerSchool_2022/main/data/MZmine/20220726_Xenobiotic_metabolism_Gapfilled_quant.csv'\n",
    "md_url <- 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/Metabolomics_SummerSchool_2022/main/data/20220726_Xenobiotic_Metabolism_metadata.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9e910",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "nft <- read.csv(nft_url, header = T, check.names = F)\n",
    "ft <- read.csv(ft_url, header = T, check.names = F)\n",
    "md <- read.csv(md_url, header = T, check.names = F, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a44d4",
   "metadata": {},
   "source": [
    "## 2) Setting a local working directory and creating an automatic result directory:\n",
    "<p style='text-align: justify;'> Works well with Jupyter Notebook. If you are working with Jupyter Notebook, you can simply copy the folder path of your input files from your local computer to the output line of the next cell. It will be set as your working directory (or working folder)</p> \n",
    "For ex: D:\\User\\Project\\Test_Data <br>\n",
    "<p style='text-align: justify;'> For Google Collab, we can upload the necessary files into a new folder using the 'Files' icon on the left and set the folder as working directory. And all the ouput files will be saved here as well and you need to download them finally into your local computer </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734f4b6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# setting the current directory as the working directory\n",
    "Directory <- normalizePath(readline(\"Enter the path of the folder with input files: \"),\"/\",mustWork=FALSE)\n",
    "setwd(Directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0436b28",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "getwd() #to get the working directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd010938",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Getting all the files in the folder\n",
    "dirs <- dir(path=paste(getwd(), sep=\"\"), full.names=TRUE, recursive=TRUE)\n",
    "folders <- unique(dirname(dirs))\n",
    "files <- list.files(folders, full.names=TRUE)\n",
    "files_1 <- basename((files))\n",
    "files_2 <- dirname((files))\n",
    "# Creating a Result folder\n",
    "dir.create(path=paste(files_2[[1]], \"_Results\", sep=\"\"), showWarnings = TRUE)\n",
    "fName <-paste(files_2[[1]], \"_Results\", sep=\"\")\n",
    "\n",
    "print(files_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab1133",
   "metadata": {},
   "source": [
    "In the following line, enter the required file index numbers separated by commas. For example as: 1,2,3. The accepted file formats are csv,txt and tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d07e82",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "input <- as.double(unlist(strsplit(readline(\"Specify the file index of gapfilled feature-file, metadata separated by commas:\"), split=\",\")))\n",
    "\n",
    "#Gets the extension of each file. Ex:csv\n",
    "pattern <- c()\n",
    "for (i in files_1){\n",
    "  sep_file <- substr(i, nchar(i)-2,nchar(i))\n",
    "  pattern <- rbind(pattern,sep_file)\n",
    "}\n",
    "#pattern\n",
    "\n",
    "ft <- read.csv(files_1[input[1]],sep = ifelse(pattern[input[1]]!=\"csv\",\"\\t\",\",\"), header=TRUE,check.names = FALSE) # By applying 'row.names = 1', the 1st column 'ID' becomes the row names\n",
    "md <-read.csv(files_1[input[2]], sep = ifelse(pattern[input[2]]!=\"csv\",\"\\t\",\",\"), header=TRUE,check.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca153ec",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> Since non gapfilled feature file is not a direct output of mzMine, not everybody will have it. Therefore, we have the next cell separately. For the imputation step, we can also impute the feature table without a non-gapfilled feature file.</p>\n",
    "\n",
    " More about this in the later section: [Imputation with a Cutoff LOD](#Impute_LOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da9db7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#If you have non gapfilled feature file:\n",
    "if(readline(\"Do you have non gap-filled feature table? Y/N:\")==\"Y\"){\n",
    "  x <- as.double(readline(\"Enter the ID number of non-gap-filled feature file:\"))\n",
    "  nft<- read.csv(files_1[x],sep=ifelse(pattern[x]!=\"csv\",\"\\t\",\",\"), header = TRUE,check.names = FALSE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12e24c",
   "metadata": {},
   "source": [
    "Lets check if the data has been read correclty!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ff705",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(ft)\n",
    "dim(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b7231",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(nft)\n",
    "dim(nft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf0865",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(md)\n",
    "dim(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19073e0",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> In the next cell, we are trying to bring the feature table and metadata in the correct format such as the rownames of metadata and column names of feature table are the same. They both are the file names and they need to be the same, as from now on, we will call the columns in our feature table based on our metadata information. Thus, using the metadata, the user can filter their data easily. You can also directly deal with your feature table without metadata by getting your hands dirty with some coding!! But having a metadata improves the user-experience greatly. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffd93c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Removing Peak area extensions\n",
    "colnames(ft) <- gsub(' Peak area','',colnames(ft))\n",
    "colnames(nft) <- gsub(' Peak area','',colnames(nft))\n",
    "md$filename<- gsub(' Peak area','',md$filename)\n",
    "\n",
    "#Removing if any NA columns present in the md file\n",
    "md <- md[,colSums(is.na(md))<nrow(md)]\n",
    "\n",
    "#Changing the row names of the files\n",
    "rownames(md) <- md$filename\n",
    "md <- md[,-1]\n",
    "rownames(ft) <- paste(ft$'row ID',round(ft$'row m/z',digits = 3),round(ft$'row retention time',digits = 3), sep = '_')\n",
    "rownames(nft) <- paste(nft$'row ID',round(nft$'row m/z',digits = 3),round(nft$'row retention time',digits = 3), sep = '_')\n",
    "\n",
    "#Picking only the files with column names containing 'mzML'\n",
    "ft <- ft[,grep('mzML',colnames(ft))]\n",
    "nft <- nft[,grep('mzML',colnames(nft))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927cf45",
   "metadata": {},
   "source": [
    "Lets check the files once again!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fb0e3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(nft)\n",
    "dim(nft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d8a48",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(ft)\n",
    "dim(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6027aea7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(md)\n",
    "dim(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bfbabc",
   "metadata": {},
   "source": [
    "# Splitting the data into Control and Samples using Metadata:\n",
    "<a id=\"data_split\"></a>\n",
    "\n",
    "For the first step: Blank removal, we need to split the data as spectra obtained from control blanks and samples respectively using the metadata. More about Blank removal in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37804f82",
   "metadata": {},
   "source": [
    "Let's call the metadata and feature table in different names so that we don't change the original files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4e9e0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Meta_Filter <- md\n",
    "input_data <- ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e56531",
   "metadata": {},
   "source": [
    "From the below cell output, we get an idea of the multiple levels in each of the metioned attributes in the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266f704",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    " lev <- c()\n",
    "  for(i in 1:ncol(Meta_Filter)){\n",
    "    x <- levels(as.factor(Meta_Filter[,i]))\n",
    "    if(is.double(Meta_Filter[,i])==T){x=round(as.double(x),2)}\n",
    "    x <-toString(x)\n",
    "    lev <- rbind(lev,x)\n",
    "  }\n",
    "\n",
    "Info_mat <- data.frame(INDEX=c(1:ncol(Meta_Filter)),ATTRIBUTES=colnames(Meta_Filter),LEVELS=lev,row.names=NULL)\n",
    "Info_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2deee4e",
   "metadata": {},
   "source": [
    "### Batch-wise splitting of the data:\n",
    "\n",
    "In our test dataset, we have used 2 species data: B.sub and E.coli. If you want to split the data batch-wise, run the next cell. For example: We can split it as B.sub + Media and/or E.coli + Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43837812",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Splitting batch-wise:\n",
    "Batch_info <- ifelse(readline(\"Do you want to split your data batch-wise? Y/N: \")==\"Y\",as.double(readline(\"Please enter the index of the attributes having batch information:\")),\"No\")\n",
    "if(is.numeric(Batch_info)==TRUE){\n",
    "    Levels_Batch <- levels(as.factor(Meta_Filter[,Batch_info]))\n",
    "    data.frame(ID=c(1:length(Levels_Batch)),Levels=Levels_Batch)\n",
    "   \n",
    "    Cdtn <-as.double(unlist(strsplit(readline(\"Enter the index number of condition(s) you want to KEEP (separated by commas):\"), split=',')))\n",
    "    \n",
    "    #Getting all the rows of metadata that satisfies each element of the condition and storing it as an element in Split_data list\n",
    "    Split_data <-list()\n",
    "    for (j in 1:length(Cdtn)){\n",
    "      Split_data[[j]] <- Meta_Filter[(Meta_Filter[,Batch_info]==Levels_Batch[Cdtn[j]]),]\n",
    "    }\n",
    "    \n",
    "    Batch_data <-do.call(rbind, Split_data) # unlists the Split data and combines them by row\n",
    "    flush.console()  \n",
    "    Sys.sleep(0.2)\n",
    "   \n",
    "    head(Batch_data)  #Visualising the Batch_data\n",
    "    dim(Batch_data)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f857ed8",
   "metadata": {},
   "source": [
    "Once we subset the data according to a batch, we can further proceed to split the control blank from the sample in the below cell. If no batch data present, you can simply split your metadata into control blank and sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09238c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#If batch data exists, it will take it as \"data\", else take Meta_filter as \"data\"\n",
    "if(exists(\"Batch_data\")==T){data <-Batch_data}else{data <-Meta_Filter}\n",
    "\n",
    "Info_mat\n",
    "Condition <- as.double(unlist(readline(\"Enter the index of the attribute to split sample and control:\")))\n",
    "\n",
    "Levels_Cdtn <- levels(as.factor(data[,Condition[1]]))\n",
    "print(matrix(Levels_Cdtn,length(Levels_Cdtn)))\n",
    " \n",
    "  #Among the shown levels of an attribute, select the ones to keep\n",
    "Ctrl_id <- as.double(unlist(readline(\"Enter the index of your BLANK:\")))\n",
    "paste0('You chosen blank is:',Levels_Cdtn[Ctrl_id])\n",
    "  \n",
    "  #Splitting the data into control and samples based on the metadata\n",
    "md_Ctrl <- data[(data[,Condition] == Levels_Cdtn[Ctrl_id]),]\n",
    "Ctrl <- input_data[,which(colnames(input_data)%in%rownames(md_Ctrl))] \n",
    "md_Samples <- data[(data[,Condition] != Levels_Cdtn[Ctrl_id]),]\n",
    "Samples <- input_data[,which(colnames(input_data)%in%rownames(md_Samples))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2a2ad",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(Ctrl)\n",
    "dim(Ctrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552dc191",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(Samples)\n",
    "dim(Samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96275453",
   "metadata": {},
   "source": [
    "# Creating a function named FrequencyPlot:  \n",
    "The below function takes the two input datatables: for example, gapfilled and non-gapfilled as its inputs, calculates the frequency distribution of the data in the order of 10 and produces a grouped barplot showing the distribution as output. The frequency plot shows where the features are present in higher number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2b3b4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#'Global' settings for plot size in the output cell\n",
    "#options(repr.plot.width=10, repr.plot.height=8,res=600) #For google collab\n",
    "options(repr.plot.width=5, repr.plot.height=3) #For Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755a764",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "FrequencyPlot <- function(x1,x2){\n",
    "  \n",
    "   #creating bins from -1 to 10^10 using sequence function seq()\n",
    "    bins <- c(-1,0,(1 * 10^(seq(0,10,1)))) \n",
    "    \n",
    "    #cut function cuts the give table into its appropriate bins\n",
    "    scores_x1 <- cut(as.matrix(x1),bins,labels = c('0','1','10','1E2','1E3','1E4','1E5','1E6','1E7','1E8','1E9','1E10')) \n",
    "    \n",
    "    #transform function convert the tables into a column format: easy for visualization \n",
    "    Table_x1<-transform(table(scores_x1)) #contains 2 columns: \"scores_x1\", \"Freq\"\n",
    "    \n",
    "    #Repeating the same steps for x2\n",
    "    scores_x2 <- cut(as.matrix(x2),bins,labels = c('0','1','10','1E2','1E3','1E4','1E5','1E6','1E7','1E8','1E9','1E10'))\n",
    "    Table_x2<-transform(table(scores_x2))\n",
    "  \n",
    "    #Getting the names of x1 and x2\n",
    "    arg1 <- deparse(substitute(x1))\n",
    "    arg2 <-deparse(substitute(x2))\n",
    "    \n",
    "    #Creating a data frame for plotting\n",
    "    data_plot <- as.data.frame(c(Table_x1$Freq,Table_x2$Freq)) #Concatenating the frequency info of both tables rowwise\n",
    "    colnames(data_plot) <- \"Freq\" #naming the 1st column as 'Freq'\n",
    "    data_plot$Condition <- c(rep(arg1,12),rep(arg2,12)) #adding a 2nd column 'Condition', which just repeats the name of x1 and x2 accordingly\n",
    "    data_plot$Range_bins <- rep(Table_x1$scores_x1,2) #Adding 3rd column 'Range Bins'\n",
    "    data_plot$Log_Freq <- log(data_plot$Freq+1) #Log scaling the frequency values\n",
    "    \n",
    "    ## GGPLOT2\n",
    "    BarPlot <- ggplot(data_plot, aes(Range_bins, Log_Freq, fill = Condition)) + \n",
    "    geom_bar(stat=\"identity\", position = \"dodge\", width=0.4) + \n",
    "    scale_fill_brewer(palette = \"Set1\") +\n",
    "    ggtitle(label=\"Frequency plot\") +\n",
    "    xlab(\"Range\") + ylab(\"(Log)Frequency\") + labs(fill = \"Data Type\") + \n",
    "    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +   # setting the angle for the x label\n",
    "    theme(axis.text.y = element_text(angle = 45, vjust = 0.5, hjust=1)) +   # setting the angle for the y label\n",
    "    theme(plot.title = element_text(hjust = 0.5)) # centering the plot title\n",
    "  \n",
    "    print(BarPlot)\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30a0c3",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> As you have noticed, the above cell didn't produce any outputs. Here, we created our own function called 'FrequencyPlot'. which works similar to \"FREQUENCY\" function in MS Excel. But we have also added codes to print a nice ggplot as ouput of the function. Hence, like 'print' function prints an output, whenever we use 'FrequencyPlot' function with the input variables, it will produce an output plot using the calculation in the above cell. </p>\n",
    "\n",
    "**Now that we have our data ready, we can start with the cleanup steps!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43099574",
   "metadata": {},
   "source": [
    "# Step1: Blank Removal\n",
    "\n",
    "<p style='text-align: justify;'> In LC-MS/MS, we use solvents called Blanks which are usually injected time-to-time to prevent carryover of the sample. The features coming from these Blanks would also be detected by LC-MS/MS instrument. Our goal here is to remove these features from our samples. The other blanks that can be removed are: Signals coming from growth media alone in terms of microbial growth experiment, signals from the solvent used for extraction methods and so on. Therefore, it is best practice to measure mass spectra of these blanks as well in addition to your sample spectra. </p>\n",
    "\n",
    "**How do we remove these blank features?** </br> \n",
    "<p style='text-align: justify;'> Since we have the feature table split into Control (Blanks) and Sample groups now, we can compare control to the sample to identify the background features coming from control. A common filtering method is to use a cutoff to remove features that are not present sufficient enough in our biological samples. </p>\n",
    "\n",
    "The steps followed in the next few cells are:\n",
    "1. <p style='text-align: justify;'> We find an average for all the feature intensities in your control set and sample set. Therefore, for n no.of features in a control or sample set, we get n no.of averaged features. </p>\n",
    "2. <p style='text-align: justify;'> Next, we get a ratio of this average_control vs average_sample. This ratio Control/sample tells us how much of that particular feature of a sample gets its contribution from control. If it is more than 30% (or Cutoff as 0.3), we consider the feature as noise. </p>\n",
    "3. <p style='text-align: justify;'> The resultant information (if ratio > Cutoff or not) is stored in a bin such as 1 = Noise or background signal, 0 = Feature Signal</p>\n",
    "4. <p style='text-align: justify;'> We count the no.of features in the bin that satisfies the condition ratio > cutoff, and consider those features as 'noise or background features' and remove them. </p>\n",
    "\n",
    "For a dataset containing several batches, the filtering steps are performed batch-wise and it can be done using the previous section [Batch-wise splitting of the data](#data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f559c14",
   "metadata": {},
   "source": [
    "**<font color='red'> The Cutoff used to obtain the all the files in MZmine Results folder is 0.3 </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160fb632",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "if(readline('Do you want to perform Blank Removal- Y/N:')=='Y'){\n",
    "    \n",
    "    #When cutoff is low, more noise (or background) detected; With higher cutoff, less background detected, thus more features observed\n",
    "    Cutoff <- as.numeric(readline('Enter Cutoff value between 0.1 & 1:')) # (i.e. 10% - 100%). Ideal cutoff range: 0.1-0.3\n",
    "    \n",
    "    #Getting mean for every feature in Ctrl and Samples\n",
    "    Avg_ctrl <- rowMeans(Ctrl, na.rm= FALSE, dims = 1) # set na.rm = FALSE to check if there are NA values. When set as TRUE, NA values are changed to 0\n",
    "    Avg_samples <- rowMeans(Samples, na.rm= FALSE, dims = 1)\n",
    "    \n",
    "    #Getting the ratio of Ctrl vs Sample\n",
    "    Ratio_Ctrl_Sample <- (Avg_ctrl+1)/(Avg_samples+1)\n",
    "    \n",
    "    # Creating a bin with 1s when the ratio>Cutoff, else put 0s\n",
    "    Bg_bin <- ifelse(Ratio_Ctrl_Sample > Cutoff, 1, 0 )\n",
    "    Blank_removal <- cbind(Samples,Bg_bin)\n",
    "\n",
    "    # Checking if there are any NA values present. Having NA values in the 4 variables will affect the final dataset to be created\n",
    "    temp_NA_Count <-cbind(Avg_ctrl ,Avg_samples,Ratio_Ctrl_Sample,Bg_bin)\n",
    "    \n",
    "    print('No of NA values in the following columns:')\n",
    "    print(colSums(is.na(temp_NA_Count)))\n",
    "\n",
    "     #Calculating the number of background features and features present\n",
    "    print(paste(\"No.of Background or noise features:\",sum(Bg_bin ==1,na.rm = TRUE)))\n",
    "    print(paste(\"No.of features after excluding noise:\",(nrow(Samples) - sum(Bg_bin ==1,na.rm = TRUE)))) \n",
    "\n",
    "    Blank_removal <- Blank_removal %>% filter(Bg_bin == 0) # Taking only the feature signals\n",
    "    Blank_removal <- as.matrix(Blank_removal[,-ncol(Blank_removal)]) # removing the last column Bg_bin \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f27df3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(Blank_removal)\n",
    "dim(Blank_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4210007",
   "metadata": {},
   "source": [
    "# Step 2: Imputation\n",
    "\n",
    "<p style='text-align: justify;'> For several reasons, real world datasets might have some missing values in it, in the form of NA, NANs or 0s. Eventhough the gapfilling step of MZmine fills the missing values, we still end up with some missing values or 0s in our feature table. This could be problematic for statistical analysis. </p> \n",
    "<p style='text-align: justify;'> In order to have a better dataset, we cannot simply discard those rows or columns with missing values as we will lose a chunk of our valuable data. Instead we can try imputing those missing values. Imputation involves replacing the missing values in the data with a meaningful, reasonable guess. There are several methods, such as: </p> \n",
    "  \n",
    "1) Mean imputation (replacing the missing values in a column with the mean or average of the column)  \n",
    "2) Replacing it with the most frequent value  \n",
    "3) Several other machine learning imputation methods such as k-nearest neighbors algorithm(k-NN), Hidden Markov Model(HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c422b8",
   "metadata": {},
   "source": [
    "## 1) Imputation using Non-gapfilled feature table:\n",
    "One such method, we are going to use is: **to replace the zeros from the gapfilled quant table with the non-gap filled table** we get from MZmine. In order to do that, we can visualize our data distribution using the frequenct plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83ae53",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "GapFilled <-Blank_removal\n",
    "if(exists(\"nft\")==T){NotGapFilled <-nft}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527abdb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "if(readline('Do you want to perform Imputation with minimum value of NonGapFilled table? - Y/N:')=='Y'){\n",
    "    plot<- FrequencyPlot(GapFilled,NotGapFilled)\n",
    "    \n",
    "    Arg1 = plot$data$Condition[1]\n",
    "    Arg2 = plot$data$Condition[13]\n",
    "    \n",
    "    # getting the 2nd minimum value of non-gap filled data. (The first minimum value in the data table is usually zero)\n",
    "    RawLOD <- round(min(NotGapFilled[NotGapFilled!=min(NotGapFilled)]))\n",
    "    \n",
    "    print(paste0(\"The minimum value greater than 0 for \",Arg1,\":\", round(min(GapFilled[GapFilled!=min(GapFilled)]))))\n",
    "    print(paste0(\"The minimum value greater than 0 for \",Arg2,\":\", RawLOD))\n",
    "\n",
    "    Imputed <- GapFilled\n",
    "    Imputed[Imputed<RawLOD] <- RawLOD # Replacing values<RawLOD with RawLOD\n",
    "    head(Imputed) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114600ea",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "write.csv(Imputed, file.path(fName,paste0('Imputed_QuantTable_filled_with_MinValue_',RawLOD,'_CutOff_Used_',Cutoff,'.csv')),row.names =TRUE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb5d3bd",
   "metadata": {},
   "source": [
    "## 2) Imputation with a Cutoff LOD:\n",
    "<a id=\"Impute_LOD\"></a>\n",
    "\n",
    "Instead of replacing with the minimum value of nft, we can also only use ft and see the frquency distribution of its features with the frequency plot. The frequency plot shows where the features are present in higher number.\n",
    "\n",
    "For ex: If until range 10-100, (shown in the figure as 1E2) there are no or very less features, we want to exclude until that range and consider from range (100-1000), or, in other words, '1E3' or '1000' as Cutoff_LOD. This value will be used to replace the zeros in the data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbafea7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "FrequencyPlot(GapFilled,GapFilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa96aab",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "if(readline('Do you want to perform Imputation with a Cutoff LOD? - Y/N:')=='Y'){\n",
    "    Cutoff_LOD <-as.numeric(readline(\"Enter your Cutoff LOD as seen in the plot:\"))  #Enter the LOD value as seen in the frequency plot\n",
    "    Imputed <- GapFilled\n",
    "    Imputed[Imputed <Cutoff_LOD] <- Cutoff_LOD\n",
    "    head(Imputed)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3eae9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dim(Imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed071fa7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "write.csv(Imputed,file.path(fName, paste0('Imputed_QuantTable_filled_with_',Cutoff_LOD,'_CutOff_Used_',Cutoff,'_Bsub','.csv')),row.names =TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f428856",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#removing all the rows with only cutoff values:\n",
    "#Imputed<-Imputed[rowMeans(Imputed)!= Cutoff_LOD,]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a2dd7",
   "metadata": {},
   "source": [
    "# Step 3:Normalization\n",
    "The following code performs sample-centric (column-wise) normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a008a7a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "if (readline(\"Do you want to perform Normalization: Y/N:\") == 'Y'){\n",
    "    #Getting column-wise sums of the input-data\n",
    "    sample_sum <- colSums(Imputed, na.rm= TRUE, dims = 1)\n",
    "    \n",
    "    #Dividing each element of a particular column with its column sum\n",
    "    Normalized_data <- c()\n",
    "    for (i in 1:ncol(Imputed)){\n",
    "        x <- Imputed[,i] / sample_sum[i]\n",
    "        Normalized_data <- cbind(Normalized_data, x)\n",
    "    }\n",
    "    colnames(Normalized_data) <- names(sample_sum)\n",
    "    \n",
    "    head(Normalized_data)\n",
    "} else return(head(Imputed))\n",
    "  \n",
    "print(paste('No.of NA values in Normalized data:',sum(is.na(Normalized_data)== TRUE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539f80c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dim(Normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc407e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "write.csv(Normalized_data, file.path(fName,'Normalised_Quant_table_Bsub_OnlyCutOffsRemoved.csv'),row.names =TRUE) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
